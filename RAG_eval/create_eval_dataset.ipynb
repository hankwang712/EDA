{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac289eef",
   "metadata": {},
   "source": [
    "# Basic Setup for Embedding & Generation Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0232b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927844dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\",\"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\"),\n",
    "    base_url=\"https://openrouter.ai/api/v1\"\n",
    ")\n",
    "\n",
    "def gen_gpt_messages(prompt):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    return messages\n",
    "def get_completion(prompt, model=\"openai/gpt-oss-120b\", temperature = 0.7):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=gen_gpt_messages(prompt),\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    if len(response.choices) > 0:\n",
    "        return response.choices[0].message.content\n",
    "    return \"generate answer error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe603170",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_completion(\"你好\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3c7bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "def siliconflow_embedding(text: str, model: str = None):\n",
    "    api_key = os.environ.get(\"SILICONFLOW_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"Please set the environment variable SILICONFLOW_API_KEY first.\")\n",
    "    if model is None:\n",
    "        model = \"BAAI/bge-m3\"\n",
    "\n",
    "    url = \"https://api.siliconflow.cn/v1/embeddings\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"input\": text\n",
    "    }\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "    res_json = response.json()\n",
    "    if \"data\" in res_json and len(res_json[\"data\"]) > 0:\n",
    "        return res_json[\"data\"][0][\"embedding\"]\n",
    "    else:\n",
    "        raise RuntimeError(f\"Failed to get embedding: {res_json}\")\n",
    "\n",
    "embedding_vector = siliconflow_embedding(\"The input text string to generate an embedding for.\")\n",
    "print(\"Embedding vector length:\", len(embedding_vector))\n",
    "print(embedding_vector[:10], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee7d53a",
   "metadata": {},
   "source": [
    "## Process Knowledge Base Documents and Insert into Chroma Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d15c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from pathlib import Path\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "\n",
    "nltk.data.path.insert(0, os.path.expanduser('~/nltk_data'))\n",
    "base_dir = Path.cwd()\n",
    "\n",
    "md_files = list(base_dir.glob(\"inputs/**/*.md\"))\n",
    "print(\"Number of Markdown files found:\", len(md_files))\n",
    "\n",
    "all_docs = []\n",
    "for file_path in md_files:\n",
    "    loader = UnstructuredMarkdownLoader(file_path)\n",
    "    docs = loader.load()\n",
    "    all_docs.extend(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100caf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "re_join_nonzh_break = re.compile(r'([^\\u4e00-\\u9fff])\\n([^\\u4e00-\\u9fff])', re.DOTALL)\n",
    "def clean_text(text: str, is_md: bool = False) -> str:\n",
    "    text = text.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "    text = re_join_nonzh_break.sub(r'\\1\\2', text)\n",
    "    text = text.replace('•', '').replace(' ', '').replace('\\u3000', '')\n",
    "    if is_md:\n",
    "        text = re.sub(r'\\n{2,}', '\\n', text)\n",
    "    return text\n",
    "\n",
    "cleaned_docs = []\n",
    "for doc in all_docs:\n",
    "    src = str(doc.metadata.get(\"source\", \"\")).lower()\n",
    "    is_md = src.endswith(\".md\")\n",
    "    doc.page_content = clean_text(doc.page_content, is_md=is_md)\n",
    "    cleaned_docs.append(doc)\n",
    "\n",
    "print(f\"Cleaning completed: {len(cleaned_docs)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2afd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import TokenTextSplitter\n",
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "def tlen(s): return len(enc.encode(s))\n",
    "\n",
    "splitter = TokenTextSplitter(\n",
    "    encoding_name=\"cl100k_base\",\n",
    "    chunk_size=1200,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "all_splits = []\n",
    "for i, doc in enumerate(cleaned_docs, start=1):\n",
    "    splits = splitter.split_documents([doc]) \n",
    "    all_splits.extend(splits)\n",
    "    print(f\"Doc {i}: {len(splits)} chunks, \"\n",
    "          f\"max tokens: {max(tlen(c.page_content) for c in splits)}\")\n",
    "\n",
    "print(f\"Total chunks: {len(all_splits)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff5c5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from typing import List\n",
    "from langchain.embeddings.base import Embeddings\n",
    "\n",
    "class SiliconFlowEmbeddings(Embeddings):\n",
    "    \"\"\"Embedding model wrapper for SiliconFlow API.\"\"\"\n",
    "\n",
    "    def __init__(self, model: str = \"BAAI/bge-m3\", api_key: str = None):\n",
    "        \"\"\"\n",
    "        Initialize the SiliconFlow Embeddings client.\n",
    "\n",
    "        Args:\n",
    "            model (str): The name of the embedding model. Default is \"BAAI/bge-m3\".\n",
    "            api_key (str): API key for SiliconFlow. If not provided, it will be read from\n",
    "                           the environment variable SILICONFLOW_API_KEY.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.api_key = api_key or os.environ.get(\"SILICONFLOW_API_KEY\")\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"Please set the SILICONFLOW_API_KEY environment variable or pass api_key explicitly.\")\n",
    "\n",
    "    def _embedding_request(self, inputs: List[str]) -> List[List[float]]:\n",
    "        url = os.environ.get(\"EMBEDDING_BASE_URL\")\n",
    "        payload = {\n",
    "            \"model\": self.model,\n",
    "            \"input\": inputs\n",
    "        }\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        response = requests.post(url, json=payload, headers=headers)\n",
    "        res_json = response.json()\n",
    "        if \"data\" in res_json and len(res_json[\"data\"]) > 0:\n",
    "            return [item[\"embedding\"] for item in res_json[\"data\"]]\n",
    "        else:\n",
    "            raise RuntimeError(f\"Failed to retrieve embeddings: {res_json}\")\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        result = []\n",
    "        batch_size = 64\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_embeddings = self._embedding_request(texts[i:i+batch_size])\n",
    "            result.extend(batch_embeddings)\n",
    "        return result\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return self.embed_documents([text])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981d53e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "embedding = SiliconFlowEmbeddings()\n",
    "persist_directory = 'data_base/vector/chroma'\n",
    "\n",
    "# If the Chroma directory already exists, you can choose to delete it manually\n",
    "# (not handled automatically in this script)\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=all_splits,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory  # This allows the vector store to be persisted to disk\n",
    ")\n",
    "\n",
    "print(f\"Number of documents stored in the vector database: {vectordb._collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef7b046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a small example case\n",
    "question = \"What should we pay attention to if we are outside during a typhoon?\"\n",
    "# Perform Max Marginal Relevance (MMR) search to retrieve top 3 diverse results\n",
    "mmr_docs = vectordb.max_marginal_relevance_search(question, k=3)\n",
    "# Print the retrieved documents\n",
    "for i, sim_doc in enumerate(mmr_docs):\n",
    "    print(f\"MMR result #{i}:\\n{sim_doc.page_content[:200]}\", end=\"\\n--------------\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc57918f",
   "metadata": {},
   "source": [
    "## RAG 性能测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983a6c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "from langchain_openai import ChatOpenAI\n",
    "api_key=os.environ.get(\"OPENAI_API_KEY\")\n",
    "base_url=os.environ.get(\"OPENAI_BASE_URL\")\n",
    "llm = ChatOpenAI(model_name=\"openai/gpt-oss-120b\", temperature=0, api_key=api_key, base_url=base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3287ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Prompt template: answer questions based on context with expert-level emergency knowledge\n",
    "template = \"\"\"You are an experienced expert in emergency management, skilled at handling all types of public emergencies.\n",
    "Based on the following context information, answer the final question:\n",
    "- If the context does not contain relevant information, explicitly say \"I don't know\". Do not make up answers.\n",
    "- Keep your response concise, accurate, and professional.\n",
    "\n",
    "{context}\n",
    "Question: {input}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# Build retriever and QA chain\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 10})\n",
    "qa_chain = (\n",
    "    RunnableParallel({\"context\": retriever, \"input\": RunnablePassthrough()})\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "results = []\n",
    "\n",
    "# Process questions and collect results\n",
    "for question in tqdm(questions, desc=\"Processing Questions\"):\n",
    "    docs = vectordb.similarity_search(question, k=10)\n",
    "    full_docs = [doc.page_content for doc in docs]\n",
    "    answer = qa_chain.invoke(question)\n",
    "    results.append({\n",
    "        \"question\": question,\n",
    "        \"top10_full_documents\": full_docs,\n",
    "        \"answer\": answer\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0586f372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "os.makedirs(\"eval\", exist_ok=True)\n",
    "with open(\"eval/qa_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94306cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import logging\n",
    "import logging.config\n",
    "import json\n",
    "from tqdm.asyncio import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import nest_asyncio\n",
    "\n",
    "from lightrag import LightRAG, QueryParam\n",
    "from lightrag.llm.openai import openai_complete_if_cache\n",
    "from lightrag.llm.ollama import ollama_embed\n",
    "from lightrag.utils import EmbeddingFunc, logger, set_verbose_debug\n",
    "from lightrag.kg.shared_storage import initialize_pipeline_status\n",
    "\n",
    "nest_asyncio.apply()\n",
    "load_dotenv(dotenv_path=\".env\", override=False)\n",
    "\n",
    "WORKING_DIR = \"./dickens\"\n",
    "OUTPUT_JSON = \"eval/lightrag_qa_results.json\"\n",
    "\n",
    "def configure_logging():\n",
    "    log_dir = os.getenv(\"LOG_DIR\", os.getcwd())\n",
    "    log_file_path = os.path.abspath(os.path.join(log_dir, \"lightrag_compatible_demo.log\"))\n",
    "    os.makedirs(os.path.dirname(log_dir), exist_ok=True)\n",
    "\n",
    "    logging.config.dictConfig({\n",
    "        \"version\": 1,\n",
    "        \"disable_existing_loggers\": False,\n",
    "        \"formatters\": {\n",
    "            \"default\": {\"format\": \"%(levelname)s: %(message)s\"},\n",
    "            \"detailed\": {\"format\": \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"},\n",
    "        },\n",
    "        \"handlers\": {\n",
    "            \"console\": {\n",
    "                \"formatter\": \"default\",\n",
    "                \"class\": \"logging.StreamHandler\",\n",
    "                \"stream\": \"ext://sys.stderr\",\n",
    "            },\n",
    "            \"file\": {\n",
    "                \"formatter\": \"detailed\",\n",
    "                \"class\": \"logging.handlers.RotatingFileHandler\",\n",
    "                \"filename\": log_file_path,\n",
    "                \"maxBytes\": int(os.getenv(\"LOG_MAX_BYTES\", 10485760)),\n",
    "                \"backupCount\": int(os.getenv(\"LOG_BACKUP_COUNT\", 5)),\n",
    "                \"encoding\": \"utf-8\",\n",
    "            },\n",
    "        },\n",
    "        \"loggers\": {\n",
    "            \"lightrag\": {\n",
    "                \"handlers\": [\"console\", \"file\"],\n",
    "                \"level\": \"INFO\",\n",
    "                \"propagate\": False,\n",
    "            },\n",
    "        },\n",
    "    })\n",
    "\n",
    "    logger.setLevel(logging.INFO)\n",
    "    set_verbose_debug(os.getenv(\"VERBOSE_DEBUG\", \"false\").lower() == \"true\")\n",
    "\n",
    "if not os.path.exists(WORKING_DIR):\n",
    "    os.mkdir(WORKING_DIR)\n",
    "\n",
    "async def llm_model_func(prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs) -> str:\n",
    "    return await openai_complete_if_cache(\n",
    "        os.getenv(\"LLM_MODEL\"),\n",
    "        prompt,\n",
    "        system_prompt=system_prompt,\n",
    "        history_messages=history_messages,\n",
    "        api_key=os.getenv(\"LLM_BINDING_API_KEY\") or os.getenv(\"OPENAI_API_KEY\"),\n",
    "        base_url=os.getenv(\"LLM_BINDING_HOST\"),\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "async def initialize_rag():\n",
    "    rag = LightRAG(\n",
    "        working_dir=WORKING_DIR,\n",
    "        llm_model_func=llm_model_func,\n",
    "        embedding_func=EmbeddingFunc(\n",
    "            embedding_dim=int(os.getenv(\"EMBEDDING_DIM\", \"1024\")),\n",
    "            max_token_size=int(os.getenv(\"MAX_EMBED_TOKENS\", \"8192\")),\n",
    "            func=lambda texts: ollama_embed(\n",
    "                texts,\n",
    "                embed_model=os.getenv(\"EMBEDDING_MODEL\", \"bge-m3:latest\"),\n",
    "                host=os.getenv(\"EMBEDDING_BINDING_HOST\", \"http://localhost:11434\"),\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "    await rag.initialize_storages()\n",
    "    await initialize_pipeline_status()\n",
    "    return rag\n",
    "\n",
    "def clean_context(context):\n",
    "    if isinstance(context, str):\n",
    "        return ' '.join(context.strip().split())\n",
    "    elif isinstance(context, list):\n",
    "        return ' '.join(' '.join(c.strip().split()) for c in context)\n",
    "    return context\n",
    "\n",
    "# English version of user prompt\n",
    "user_prompt = \"\"\"You are an experienced expert in emergency management, skilled in handling all kinds of public emergencies.\n",
    "Based on the following contextual information, answer the question at the end:\n",
    "- If the context does not contain relevant information, explicitly respond with \"I don't know\" — do not make up answers.\n",
    "- Use concise, accurate, and professional language in your answer.\n",
    "\"\"\"\n",
    "\n",
    "async def main():\n",
    "    results = []\n",
    "    rag = await initialize_rag()\n",
    "    await rag.aclear_cache()\n",
    "\n",
    "    for question in tqdm(questions, desc=\"Processing Questions\"):\n",
    "        try:\n",
    "            context_response = await rag.aquery(\n",
    "                question,\n",
    "                param=QueryParam(\n",
    "                    mode=\"hybrid\",\n",
    "                    stream=False,\n",
    "                    user_prompt=user_prompt,\n",
    "                    only_need_context=True,\n",
    "                    top_k=10,\n",
    "                ),\n",
    "            )\n",
    "            full_response = await rag.aquery(\n",
    "                question,\n",
    "                param=QueryParam(\n",
    "                    mode=\"hybrid\",\n",
    "                    stream=False,\n",
    "                    user_prompt=user_prompt,\n",
    "                    only_need_context=False,\n",
    "                    top_k=10,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            results.append({\n",
    "                \"question\": question,\n",
    "                \"context\": clean_context(context_response),\n",
    "                \"llm_output\": full_response,\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                \"question\": question,\n",
    "                \"context\": \"\",\n",
    "                \"llm_output\": f\"[ERROR] {str(e)}\",\n",
    "            })\n",
    "\n",
    "    await rag.finalize_storages()\n",
    "\n",
    "    with open(OUTPUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"\\nAll questions processed. Results saved to: {OUTPUT_JSON}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    configure_logging()\n",
    "    asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d92bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def clean_context_text(text):\n",
    "    \"\"\"\n",
    "    Clean the context string by removing unnecessary fields and formatting artifacts.\n",
    "    \"\"\"\n",
    "    text = re.sub(r\"-----.*?-----\", \"\", text)            \n",
    "    text = re.sub(r\"```json\", \"\", text)                   \n",
    "    text = re.sub(r\"```\", \"\", text)\n",
    "    \n",
    "    fields_to_remove = [\"created_at\", \"file_path\", \"type\"]\n",
    "    for field in fields_to_remove:\n",
    "        text = re.sub(rf'\"{field}\"\\s*:\\s*\".*?\"\\s*,?', \"\", text)\n",
    "    \n",
    "    text = re.sub(r\",\\s*([}\\]])\", r\"\\1\", text)           \n",
    "    text = re.sub(r\"\\n\\s*\\n\", \"\\n\", text)                  \n",
    "    return text.strip()\n",
    "\n",
    "def clean_context_fields_in_place(input_file):\n",
    "    \"\"\"\n",
    "    Load a JSON file and clean the 'context' field of each item in-place.\n",
    "    \"\"\"\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for item in data:\n",
    "        ctx = item.get(\"context\", \"\")\n",
    "        if isinstance(ctx, str):\n",
    "            item[\"context\"] = clean_context_text(ctx)\n",
    "\n",
    "    with open(input_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"'context' field cleaned and written back to: {input_file}\")\n",
    "clean_context_fields_in_place(\"eval/lightrag_qa_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c33a54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
