# ================================
# System / Network (optional)
# RAGAS may download models via Hugging Face; in mainland China you may need a proxy.
# requests will auto-read HTTP(S)_PROXY from env.
# ================================
HTTP_PROXY=http://YOUR_PROXY_HOST:YOUR_PROXY_PORT
HTTPS_PROXY=http://YOUR_PROXY_HOST:YOUR_PROXY_PORT


# ================================
# Embedding Service (SiliconFlow)
# Your code expects these names.
# ================================
SILICONFLOW_API_KEY=your_siliconflow_api_key
EMBEDDING_BASE_URL=https://api.siliconflow.cn/v1/embeddings
# Model name is hard-coded to "BAAI/bge-m3" in code; change in code if needed.


# ================================
# Evaluation LLM (OpenAI-compatible)
# ================================
LLM_MODEL_NAME=gpt-4o
OPENAI_API_KEY=your_openai_api_key
OPENAI_BASE_URL=https://api.openai.com/v1
MAX_COMPLETION_TOKENS=8192


# ================================
# Logging Configuration
# ================================
LOG_DIR=./logs                           # Default is current directory
LOG_MAX_BYTES=10485760                  # 10 MB per log file
LOG_BACKUP_COUNT=5                      # Max number of rotated log files
VERBOSE_DEBUG=false                     # Set to "true" to enable verbose debug logs


# ================================
# LightRAG Runtime LLM + Embedding
# ================================
LLM_MODEL=gpt-4o                         # Used by LightRAG for completion
LLM_BINDING_API_KEY=your_openai_api_key # Optional override for LLM API key
LLM_BINDING_HOST=https://api.openai.com/v1

EMBEDDING_MODEL=bge-m3:latest           # Ollama-compatible embedding model
EMBEDDING_DIM=1024                      # Vector dimension
MAX_EMBED_TOKENS=8192                   # Max token length for embedding input
